import asyncio
import sys
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
import asyncio
import csv
import os
import sys
from typing import List, Set
from pathlib import Path

from dotenv import load_dotenv

# Charger les variables d'environnement AVANT tout import crawl4ai
load_dotenv()

# === CONFIGURATION ===
try:
    from config import BASE_URL, CSS_SELECTOR, REQUIRED_KEYS
except ImportError:
    print("âŒ Fichier config.py manquant.")
    sys.exit(1)

# === UTILITAIRES LOCAUX (fallback si utils/ absent) ===

def save_to_csv(data: List[dict], filename: str):
    """Sauvegarde en CSV avec encodage UTF-8-BOM pour Excel FR"""
    if not data:
        print("âš ï¸ Aucune donnÃ©e Ã  sauvegarder.")
        return
    filepath = Path(filename)
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=REQUIRED_KEYS, extrasaction="ignore")
        writer.writeheader()
        writer.writerows(data)
    print(f"âœ… {len(data)} Ã©lÃ©ments sauvegardÃ©s dans '{filepath}'.")


def is_complete(item: dict, keys: List[str]) -> bool:
    return all(item.get(k) and str(item[k]).strip() for k in keys)


def is_duplicate(modele: str, seen: Set[str]) -> bool:
    return modele.strip().lower() in {m.lower() for m in seen}


# === IMPORTS PRINCIPAUX ===
try:
    from crawl4ai import AsyncWebCrawler
    from utils.scraper_utils import get_browser_config, get_llm_strategy, fetch_and_process_page
except ImportError as e:
    print(f"âŒ Erreur d'import : {e}")
    print("Veuillez vÃ©rifier que 'utils/scraper_utils.py' existe.")
    sys.exit(1)


# === FONCTION PRINCIPALE ===
async def crawl_all_vetements():
    print("ğŸš€ DÃ©marrage du scraping des vÃªtements de skiâ€¦")
    print(f"Base URL : {BASE_URL}")
    print(f"SÃ©lecteur CSS : '{CSS_SELECTOR}'")
    print("-" * 50)

    # ğŸ”‘ VÃ©rification clÃ© API
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("âŒ Erreur : OPENROUTER_API_KEY manquante dans .env")
        sys.exit(1)

    # ğŸ”§ Configuration
    browser_config = get_browser_config()
    llm_strategy = get_llm_strategy(REQUIRED_KEYS)
    session_id = "ski_crawl_2025"

    all_items: List[dict] = []
    seen_modeles: Set[str] = set()
    page = 1
    max_pages = 50  # SÃ©curitÃ© anti-boucle infinie

    async with AsyncWebCrawler(**browser_config) as crawler:
        while page <= max_pages:
            print(f"\nğŸ“„ Traitement de la page {page}â€¦")

            items, should_stop = await fetch_and_process_page(
                crawler=crawler,
                numero_page=page,
                base_url=BASE_URL,
                css_selector=CSS_SELECTOR,
                llm_strategy=llm_strategy,
                session_id=session_id,
                required_keys=REQUIRED_KEYS,
                noms_vus=seen_modeles,
            )

            if items:
                all_items.extend(items)
                print(f"ğŸ“ˆ +{len(items)} â†’ total : {len(all_items)}")

            if should_stop:
                print("ğŸ›‘ Pagination terminÃ©e (plus de rÃ©sultats).")
                break

            if not items and page > 1:
                print("âš ï¸ Page vide aprÃ¨s la 1Ã¨re â†’ fin probable.")
                break

            page += 1
            await asyncio.sleep(2.5)  # Respect du serveur

        # RÃ©sumÃ©
        print("\n" + "=" * 50)
        print(f"âœ… Scraping terminÃ©.")
        print(f"ğŸ“¦ {len(all_items)} vÃªtements collectÃ©s.")
        print(f"ğŸ” {len(seen_modeles)} modÃ¨les uniques.")

        # Sauvegarde
        if all_items:
            filename = "exports/vetements_ski_2025.csv"
            save_to_csv(all_items, filename)

            # Stats simples
            prix_valides = [
                float(v["prix"].replace("â‚¬", "").replace(",", ".").strip())
                for v in all_items
                if v["prix"].replace("â‚¬", "").replace(",", ".").replace(".", "").isdigit()
            ]
            if prix_valides:
                print(f"ğŸ’¶ Prix moyen : {sum(prix_valides) / len(prix_valides):.2f} â‚¬")
                print(f"ğŸ“‰ Min : {min(prix_valides):.2f} â‚¬ | ğŸ“ˆ Max : {max(prix_valides):.2f} â‚¬")

        else:
            print("âŒ Aucun vÃªtement nâ€™a Ã©tÃ© extrait. VÃ©rifiez :")
            print("   - Le sÃ©lecteur CSS (`li.product-item` ?)")
            print("   - Lâ€™URL de base (pagination ? `&page=2` ?)")
            print("   - La clÃ© OpenRouter (testez avec `curl` si besoin)")

        # Affiche l'usage LLM (si supportÃ©)
        try:
            if hasattr(llm_strategy, "_llm_client") and hasattr(llm_strategy._llm_client, "show_usage"):
                llm_strategy._llm_client.show_usage()
        except:
            pass


# === POINT Dâ€™ENTRÃ‰E ===
async def main():
    try:
        await crawl_all_vetements()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Interruption utilisateur. ArrÃªt gracieux.")
    except Exception as e:
        print(f"\nğŸ’¥ Erreur critique : {e}")
        import traceback
        traceback.print_exc()




# --- FIN DE main.py ---

if __name__ == "__main__":
    # ğŸ”§ Correction Windows : force une boucle compatible subprocess
    if sys.platform == "win32":
        try:
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        except AttributeError:
            # Si Proactor n'est pas dispo (ex: WSL1), on force Selector
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # ğŸ” ExÃ©cute dans une boucle propre, sans dÃ©pendre de l'IDE
    try:
        asyncio.run(main())
    except RuntimeError as e:
        if "event loop is closed" in str(e):
            # Contournement pour certains IDE (ex: Spyder)
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(main())
            finally:
                loop.close()
        else:
            raise
   